import numpy as np
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense, Embedding, SimpleRNN
from keras.preprocessing import sequence
from tensorflow.keras.preprocessing.sequence import pad_sequences
[2]
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz
17464789/17464789 [==============================] - 3s 0us/step

[3]
max_review_length = 500
x_train = pad_sequences(x_train, maxlen=max_review_length)
x_test = pad_sequences(x_test, maxlen=max_review_length)
[5]
model=Sequential()
model.add(Embedding(10000, 32))
model.add(SimpleRNN(32))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=5, batch_size=64)
Epoch 1/5
391/391 [==============================] - 272s 669ms/step - loss: 0.6150 - accuracy: 0.6616
Epoch 2/5
391/391 [==============================] - 310s 792ms/step - loss: 0.4712 - accuracy: 0.7914
Epoch 3/5
391/391 [==============================] - 247s 632ms/step - loss: 0.3048 - accuracy: 0.8767
Epoch 4/5
391/391 [==============================] - 304s 778ms/step - loss: 0.1955 - accuracy: 0.9283
Epoch 5/5
391/391 [==============================] - 282s 723ms/step - loss: 0.1028 - accuracy: 0.9672

<keras.callbacks.History at 0x242a1171e80>
test_loss, test_acc = model.evaluate(x_test, y_test)
print('Test accuracy:', test_acc)
780/782 [============================>.] - ETA: 0s - loss: 0.4840 - accuracy: 0.8270
pred = model.predict(x_test)
 def review(x):
     word_index = imdb.get_word_index()
     reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
     decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x])
     print(decoded_review)

     if pred[0]>.5:
         print('positive')
     else:
         print('negative')
         review(x_test[60])
